{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing) 자연어 처리란"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어\n",
    "- 사람이 사용하는 고유한 언어\n",
    "- 인공언어의 반대 의미\n",
    "    - 인공언어: 특정 목적을 위해 인위적으로 만든 언어\n",
    "    - ex) 프로그래밍 언어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리\n",
    "- 사람이 사용하는 자연어를 컴퓨터가 사용할 수 있도록 처리하는 과정.\n",
    "- 자연어 처리 응용분야\n",
    "    - 번역 시스템\n",
    "    - 문서요약\n",
    "    - 감성분석\n",
    "    - 대화형 시스템(챗봇)\n",
    "    - 정보 검색 시스템\n",
    "    - 텍스트 마이닝\n",
    "    - 음성인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분석 수행 프로세스\n",
    "\n",
    "1. 텍스트 전처리 \n",
    "    - 클렌징(cleansing)\n",
    "        - 특수문자, 기호 필요없는 문자 제거\n",
    "        - 대소문자 변경\n",
    "    - stop word(분석에 필요 없는 토큰) 제거\n",
    "    - 텍스트 토큰화\n",
    "        - 분석의 최소단위로 나누는 작업\n",
    "        - 보통 단어단위나 글자단위로 나눈다.\n",
    "    - 어근 추출(Stemming/Lemmatization)을 통한 텍스트 정규화 작업\n",
    "2. Feature vectorization   \n",
    "    - 문자열 비정형 데이터인 텍스트를 숫자타입의 정형데이터로 만드는 작업\n",
    "    - BOW와 Word2Vec\n",
    "3. 머신러닝 모델 수립, 학습, 예측, 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "- Natural Language ToolKit\n",
    "- https://www.nltk.org/\n",
    "- 자연어 처리를 위한 파이썬 패키지\n",
    "\n",
    "## NLTK 설치\n",
    "- nltk 패키지 설치\n",
    "pip 설치\n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "conda 설처\n",
    "```\n",
    "conda install nltk\n",
    "```\n",
    "\n",
    "- NLTK 추가 패키지 설치\n",
    "```python\n",
    "import nltk\n",
    "nltk.download() # 설치 GUI 프로그램 실행\n",
    "nltk.download('패키지명')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/com/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2019.8.28  |                0         133 KB\n",
      "    certifi-2019.9.11          |           py37_0         154 KB\n",
      "    conda-4.8.0                |           py37_1         2.8 MB\n",
      "    openssl-1.1.1d             |       h1de35cc_2         2.2 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         5.3 MB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates                                  anaconda --> pkgs/main\n",
      "  certifi                                       conda-forge --> pkgs/main\n",
      "  conda                                         conda-forge --> pkgs/main\n",
      "  openssl                                          anaconda --> pkgs/main\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "conda-4.8.0          | 2.8 MB    | ##################################### | 100% \n",
      "certifi-2019.9.11    | 154 KB    | ##################################### | 100% \n",
      "ca-certificates-2019 | 133 KB    | ##################################### | 100% \n",
      "openssl-1.1.1d       | 2.2 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 주요기능\n",
    "- ### 말뭉치(corpus) 제공\n",
    "    - **말뭉치**: 언어 연구를 위해 텍스트를 컴퓨터가 읽을 수 있는 형태로 모아 놓은 언어 자료를 말한다.\n",
    "    - 예제용 말뭉치 데이터를 제공한다.\n",
    "corpus라는 단어는 기억해둘 필요가 있다.\n",
    "- ### 텍스트 정규화를 위한 기능 제공\n",
    "    - 토큰 생성\n",
    "    - 여러 언어의 Stop word(불용어) 제공\n",
    "    - 형태소 분석\n",
    "        - 형태소\n",
    "            - 의미가 있는 가장 작은 말의 단위\n",
    "        - 형태소 분석\n",
    "            - 말뭉치에서 의미있는(분석시 필요한) 형태소들만 추출하는 것           \n",
    "        - 어간추출(Stemming)\n",
    "        - 원형복원(Lemmatization)\n",
    "        - 품사부착(POS tagging - Part Of Speech)\n",
    "- ### 분석기능을 제공해 주는 클래스 제공\n",
    "    - Text\n",
    "    - FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T13:34:51.695978Z",
     "start_time": "2019-12-21T13:34:17.682252Z"
    }
   },
   "outputs": [],
   "source": [
    "!conda install -y -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK 텍스트 정규화 기본 문법\n",
    "\n",
    "## 텍스트 토큰화\n",
    "- 분석을 위해 문장을 작은 단위로 나누는 작업.\n",
    "- **토큰(Token)**\n",
    "    - 나뉜 문자열의 단위를 말한다. \n",
    "    - 정하기에 따라 문장, 단어일 수도 있고 문자일 수도 있다. \n",
    "- **Tokenizer**\n",
    "    - 문장을 token으로 나눠주는 함수\n",
    "    - NLTK 주요 tokenizer\n",
    "        - **sent_tokenize()** : 문장단위로 나눠준다.\n",
    "        - **word_tokenize()** : 단어단위로 나눠준다.\n",
    "        - **regexp_tokenize()** : 토큰의 단위를 정규표현식으로 지정\n",
    "        - 반환타입 : 토큰하나 하나를 원소로 하는 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"\"\"Beautiful is better than ugly.\n",
    "Explicit is better than implicit.\n",
    "Simple is better than complex.\n",
    "Complex is better than complicated.\n",
    "Flat is better than nested.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokenize() : 문장단위로 토큰화. 기준: '.'\n",
    "sentence_tokens = nltk.sent_tokenize(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beautiful is better than ugly.',\n",
       " 'Explicit is better than implicit.',\n",
       " 'Simple is better than complex.',\n",
       " 'Complex is better than complicated.',\n",
       " 'Flat is better than nested.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어단위로 토큰화 : 기준: 공백\n",
    "word_tokens = nltk.word_tokenize(text_sample)\n",
    "len(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beautiful',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'ugly',\n",
       " '.',\n",
       " 'Explicit',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'implicit',\n",
       " '.',\n",
       " 'Simple',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'complex',\n",
       " '.',\n",
       " 'Complex',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'complicated',\n",
       " '.',\n",
       " 'Flat',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'nested',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regexp_tokenize() - 토큰단위를 정규표현식으로 지정.\n",
    "t = \"a1b2c3d\"\n",
    "nltk.regexp_tokenize(t, r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \\w : 글자, 숫자, _\n",
    "reg_tokens = nltk.regexp_tokenize(text_sample, '\\w+') #r'\\w') \n",
    "len(reg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beautiful',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'ugly',\n",
       " 'Explicit',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'implicit',\n",
       " 'Simple',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'complex',\n",
       " 'Complex',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'complicated',\n",
       " 'Flat',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'nested']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword (불용어)\n",
    "- 문장내에서는 자주 사용되지만 분석시 필요없는 단어들을 말함.\n",
    "    - ex) 조사, 접미사, 접속사, 대명사 등\n",
    "- Stopword 단어들을 List로 묶어서 관리한다.\n",
    "    - nltk의 미리 정의된 리스트를 이용하거나 직접 정의할 수 있음.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "# nltk가 지원하는 stopword 언어 조회\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "type(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장별 토큰화 함수 구현\n",
    "- 문장별 토큰화 시키는 함수 구현\n",
    "- 쉼표,마침표등 구두점(punctuation)은 공백처리한다.\n",
    "- stopword를 제외한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize_text(document):\n",
    "    stop_words = stopwords.words('english')\n",
    "    #1. 텍스트 클린징 - 소문자로 변환, 특수문자 제거.\n",
    "    document = document.lower()\n",
    "    # 특수문자를 공백으로 변환\n",
    "    pattern = '[{}]'.format(string.punctuation)\n",
    "    sentence_token = nltk.sent_tokenize(document)\n",
    "    sent_list = [] # 결과를 담을 리스트\n",
    "    for sentence in sentence_token:\n",
    "        sentence = re.sub(pattern, ' ', sentence) #특수문자들을 공백으로 전환\n",
    "        word_token = nltk.word_tokenize(sentence)\n",
    "        # stopword를 제거\n",
    "        word_token = [word for word in word_token if word not in stop_words]\n",
    "        sent_list.append(word_token)\n",
    "        \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beautiful', 'better', 'ugly'],\n",
       " ['explicit', 'better', 'implicit'],\n",
       " ['simple', 'better', 'complex'],\n",
       " ['complex', 'better', 'complicated'],\n",
       " ['flat', 'better', 'nested']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_text(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[{}]'.format(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T14:40:43.867250Z",
     "start_time": "2019-12-21T14:40:43.862275Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenize_text(text_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    "- 형태소\n",
    "    - 일정한 의미가 있는 가장 작은 말의 단위\n",
    "- 형태소 분석  \n",
    "    - 말뭉치에서 의미있는(분석에 필요한) 형태소들만 추출하는 것\n",
    "    - 보통 단어로 부터 어근, 접두사, 접미사, 품사등 언어적 속성을 파악하여 처리한다. \n",
    "- 형태소 분석을 위한 기법\n",
    "    - 어간추출(Stemming)\n",
    "    - 원형(기본형) 복원 (Lemmatization)\n",
    "    - 품사부착 (POS tagging - Part Of Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간추출(Stemming)\n",
    "- 어간: 활용어에서 변하지 않는 부분\n",
    "    - painted, paint, painting => 어간: paint\n",
    "- 어간 추출 \n",
    "    - 단어에서 어미를 제거하고 어간을 추출하는 작업\n",
    "- nltk의 주요 어간 추출 알고리즘\n",
    "    - Porter Stemmer\n",
    "    - Lancaster Stemmer\n",
    "    - Snowball Stemmer\n",
    "- 메소드\n",
    "    - `stemmer객체.stem(단어)`\n",
    "- Stemming은 규칙기반으로 어간을 찾기 때문에 철자가 훼손된 어근 단어를 추출하는 경우도 있다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"works\", \"working\", \"worked\", \"painting\", \"painted\", \"happy\", \"happier\", \"happiest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work', 'work', 'work', 'paint', 'paint', 'happi', 'happier', 'happiest']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stem = PorterStemmer()\n",
    "[porter_stem.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work', 'work', 'work', 'paint', 'paint', 'happy', 'happy', 'happiest']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_stem = LancasterStemmer()\n",
    "[l_stem.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work', 'work', 'work', 'paint', 'paint', 'happi', 'happier', 'happiest']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb_stem = SnowballStemmer(\"english\")\n",
    "[sb_stem.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원형(기본형)복원(Lemmatization)\n",
    "- 단어의 기본형을 반환한다.\n",
    "    - ex) am, is, are => be\n",
    "- 단어의 품사를 지정하면 정확한 결과를 얻을 수 있다. \n",
    "- `WordNetLemmatizer객체.lemmatize(단어 [, pos=품사])`\n",
    "\n",
    "> 어간추출과 원형복원은 문법적 또는 의미적으로 변한 단어의 원형을 찾는 역할은 동일하다.<br>\n",
    "> **원형복원**은 품사와 같은 문법적요소와 문장내에서의 의미적인 부분을 감안해 찾기 때문에 어간추출 방식보다 더 정교하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'be', 'be', 'have', 'have', 'have']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['is', 'am', 'are', 'had', 'has', 'have']\n",
    "# [lemm.lemmatize(word) for word in words]\n",
    "[lemm.lemmatize(word, pos='v') for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meet'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm.lemmatize('meeting', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meeting'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm.lemmatize('meeting', pos='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사부착-POS Tagging(Part-Of-Speech Tagging)\n",
    "- 형태소에 품사를 붙이는 작업.\n",
    "    - 품사의 구분이나 표현은 언어, 학자마다 다르다. \n",
    "- NLTK는 펜 트리뱅크 태그세트(Penn Treebank Tagset) 이용\n",
    "    - NN: 명사\n",
    "    - NNP: 고유명사\n",
    "    - JJ: 형용사\n",
    "    - VB: 동사\n",
    "    - VBP : 동사 현재형\n",
    "    - `nltk.help.upenn_tagset(['키워드'])` \n",
    "- `pos_tag(단어)`    \n",
    "    - 단어와 품사를 튜플로 묶은 리스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/com/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 'NN'),\n",
       " ('car', 'NN'),\n",
       " ('have', 'VBP'),\n",
       " ('Korea', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('곰', 'NNP'),\n",
       " ('가자', 'NNP'),\n",
       " ('간다', 'NN')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['dog', 'car','have','Korea','student', '곰', '가자', '간다'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 'NN'),\n",
       " ('car', 'NN'),\n",
       " ('have', 'VBP'),\n",
       " ('Korea', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('go', 'VB')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = pos_tag(['dog','car','have','Korea','student','is','go'])\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'car', 'Korea', 'student']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = ['NN', 'NNP', 'NNS',' NNPS']\n",
    "[word for word, pos in pos_list if pos in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전처리 프로세스\n",
    "- 클렌징(cleansing)\n",
    "    - 특수문자, 기호 필요없는 문자 제거\n",
    "    - 대소문자 변경\n",
    "- stop word(분석에 필요 없는 토큰) 제거\n",
    "- 텍스트 토큰화\n",
    "    - 분석의 최소단위로 나누는 작업\n",
    "    - 보통 단어단위나 글자단위로 나눈다.\n",
    "- 어근 추출(Stemming/Lemmatization)을 통한 텍스트 정규화 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원형복원 - stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def tokenize_text2(document):\n",
    "    stop_words = stopwords.words('english')\n",
    "    #1. 텍스트 클린징 - 소문자로 변환, 특수문자 제거.\n",
    "    document = document.lower()\n",
    "    # 특수문자를 공백으로 변환\n",
    "    pattern = '[{}]'.format(string.punctuation)\n",
    "    sentence_token = nltk.sent_tokenize(document)\n",
    "    sent_list = [] # 결과를 담을 리스트\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    for sentence in sentence_token:\n",
    "        sentence = re.sub(pattern, ' ', sentence) #특수문자들을 공백으로 전환\n",
    "        word_token = nltk.word_tokenize(sentence)\n",
    "        # stopword를 제거\n",
    "        word_token = [word for word in word_token if word not in stop_words]\n",
    "        \n",
    "        # stemming\n",
    "        word_token = [stemmer.stem(word) for word in word_token]\n",
    "        \n",
    "        sent_list.append(word_token)\n",
    "        \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beauti', 'better', 'ugli'],\n",
       " ['explicit', 'better', 'implicit'],\n",
       " ['simpl', 'better', 'complex'],\n",
       " ['complex', 'better', 'complic'],\n",
       " ['flat', 'better', 'nest']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_text2(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news.txt', 'rt', encoding='cp949') as f:\n",
    "    news_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sitting in his parents’ hair salon, Min-Kyu is buried in his computer game.\\nWearing his gray soccer '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sit',\n",
       "  'parent',\n",
       "  '’',\n",
       "  'hair',\n",
       "  'salon',\n",
       "  'min',\n",
       "  'kyu',\n",
       "  'buri',\n",
       "  'comput',\n",
       "  'game'],\n",
       " ['wear',\n",
       "  'gray',\n",
       "  'soccer',\n",
       "  'kit',\n",
       "  'boot',\n",
       "  'constantli',\n",
       "  'click',\n",
       "  'tile',\n",
       "  'floor',\n",
       "  '11',\n",
       "  'year',\n",
       "  'old',\n",
       "  'lost',\n",
       "  'parallel',\n",
       "  'univers',\n",
       "  'far',\n",
       "  'distract',\n",
       "  'nois',\n",
       "  'electr',\n",
       "  'clipper',\n",
       "  'faint',\n",
       "  'smell',\n",
       "  'hairspray'],\n",
       " ['seem',\n",
       "  'happen',\n",
       "  'often',\n",
       "  'part',\n",
       "  'world',\n",
       "  'new',\n",
       "  'malden',\n",
       "  'suburban',\n",
       "  'southwest',\n",
       "  'london',\n",
       "  'ear',\n",
       "  'prick',\n",
       "  'name',\n",
       "  '“',\n",
       "  'mr'],\n",
       " ['son', '”', 'mention'],\n",
       " ['mr',\n",
       "  'son',\n",
       "  'affection',\n",
       "  'known',\n",
       "  'actual',\n",
       "  'son',\n",
       "  'heung',\n",
       "  'min',\n",
       "  'south',\n",
       "  'korean',\n",
       "  'tottenham',\n",
       "  'footbal',\n",
       "  'star',\n",
       "  'nation',\n",
       "  'hero']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ' \"\n",
    "news_words = tokenize_text2(news_text)\n",
    "news_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석을 위한 클래스들\n",
    "## Text클래스\n",
    "- 문서 분석에 유용한 여러 메소드 제공\n",
    "- **토큰 리스트**을 입력해 객체생성 후 제공되는 메소드를 이용해 분석한다.\n",
    "- 주요 메소드\n",
    "    - count(토큰'): Text 내에서 매개변수로 전달한 토큰의 개수 반환\n",
    "    - plot(rank:int): 단어의 빈도수를 선그래프로 시각화\n",
    "    - dispersion_plot([토큰리스트]): 리스트의 토큰들이 문서내 나타난 위치 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreqDist\n",
    "- document에서 사용된 토큰(단어)의 사용빈도 데이터를 가지는 클래스\n",
    "    - 토큰(단어)를 key, 개수를 value로 가지는 딕셔너리 형태\n",
    "- 생성\n",
    "    - Text 객체의 vocab() 메소드로 조회한다.\n",
    "    - 생성자(Initializer)에 토큰 List를 직접 넣어 생성가능\n",
    "- 주요 메소드\n",
    "    - N(): 총 단어수\n",
    "    - get(단어) 또는 FreqDist['단어'] : 특정 단어의 출연 빈도수\n",
    "    - freq(단어): 총 단어수 대비 특정단어의 출연비율\n",
    "    - most_common() : 빈도수 순서로 정렬하여 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn의 CountVectorizer를 이용해 TDM 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## news에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
